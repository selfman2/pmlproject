Once you develop your model, you'll use it to predict the class of each of the 20 records in the test file.  Then, you will create 20 separate .txt files, each containing the prediction for one and only one test record.  So each file will only contain one line, and on that one line, there will be only a single character - the letter representing your predicted class for that test record.  You will then submit each of these files through the course website.

The code on the submission page allows you to quickly generate the 20 .txt files assuming you have all of your 20 predictions stored in a dataframe or vector.

------------------

One thing I didn't understand from the lectures is that caret's train() function is optimizing model parameters during its run.  That's why it takes so long.  For example method="glmnet" has two parameters -- a regularization term and a blending term.  The train() function (somehow) first picks a range for each parameter.  I think it picks 3 values for each by default.  It then estimates accuracy by boosting, k-fold cross-validation or whatever. 

For my example with glmnet, say we specify 10-fold cross-validation, repeated 3 times.  That means train() builds 10 folds * 3 repeats * 3^2 parameter combinations = 270 models.  It then selects the best one.  I believe the final model is retrained with all the samples before it is returned. 

If you want a good set of parameters, you would somehow need to do what train() does.  You could do that by creating a training set to build models, a validation set to evaluate parameters, and a testing set for final error estimation.  Or you could build a training set and a testing set, let train() select parameters with cross-validation, and again use the testing set for final evaluation

---------------------


Make sure that the data class for each predictor that you decided to keep is the same in each data set as some of the variables have different classes between the two data sets. Further, if you chose to include any factor variables as predictors then check that they have the same number of factor levels. It seems that predict.randomForest does not work if these two conditions are not met.

You might review the output of str(training) and str(testing).  Comparing them will show if the type of predictor matches for each variable.

----------------------

I believe the "out-of-sample error" is the error rate on data that was NOT used to tune or train the model. 

The formula you suggest, Error = 1 - Accuracy, is the one that makes sense to me

----------------------

Shoko,

Same for me.....did you keep the participants name in the dataset? Ordinarily that would scream "over fitting" to me but in this case it was in the 20 sample data set so I included it. I wonder what would happen without that variable in the data set?

In real life, if I was trying to make a good forecast of proper technique, I would never make a model to forecast an individual's performance. 

The objective in this case was a good grade sooooooo...... :)

--

if you look at the data from the test dataset to pick the predictors, you are intentionally over-fitting your model.

In my case, I only conserved data from the device sensors and I got the 20 samples good. In fact, I did not even look at the test dataset. A good pre-processing and the good algorithm can do the job !

-----------------------------

does this model make sense?

> modelFitRPart <- train(classe ~ ., data=red, method="rpart")
> print(modelFitRPart$finalModel)

Well, pitch_forearm is only below -33.95 for Carlitos, Jeremy and Eurico, and almost only when they do a correctly executed exercise (class A), just few class B's in there. It's never below -33.95 for Adelmo, Pedro, or Charles. So it is an almost perfect classifier for the three guys, but doesn't help with three others at all. 


As for that measurement means, that's pretty hard to deduce from the data and paper.. From a non-expert guess, 0 forearm pitch would mean that the arm (and the sensor) is parallel to torso, appropriate for doing a bicep curl, and negative or positive values would mean the arm is lifted forwards or backwards. But that is not the case, as Jeremy does class A at either -34 forearm_pitch or below, or at above 55 or so, class B at something like -20 to -34 or +40 to -55, class E around 0 (I'm just estimating from a plot). Adelmo's pitch_forearm, on the other hand, is always exactly 0, and probably should be NA - it's not like his forearm only move undetectably small amounts. Others are different in their own unique ways.


look at the accuracy if it makes sense

------------------------------

out-of-sample-error/cross-validation

I've used a random forest and was able to predict all the 20 test values correctly an my confusion matrix on the test data looks like this:

                     Reference
Prediction    A    B    C    D    E
         A 1394    3    0    0       0
         B    1  946    1    0        0
         C    0    0  854    6        0
         D    0    0    0   797        0
         E    0    0    0     1      901
---

It's a little bit confusing, as the lectures on cross-validation and random forests are not all that explicit about validation strategies, and the 20-case test data is bit odd. Random forest automatically samples the data for each forest run and estimates the out-of-bag error for those left out - i.e. random forests has cross-validation built in to help choose between models. Some would call the data used to choose between models validation data, I guess, instead of test data. You can calculate your true out-of-sample error from this table, as this data was not used to make any choices in the model. You did cross-validation as part of your random forest modeling, and you did it by estimating the out-of-sample error on unused data too, depending on definition.

------------------------------------------

look at histogram: exclude variables? log-transform? add any other variable to discriminate better?

I have not had the time to dig in and look at all of the variables univariately like this.  However, dont be mislead into thinking that because you dont have a pretty histogram that something needs to be done.  In fact in classification problems like this, patterns like that may actually be caused by the different techniques of the user and those two shapes may very well be showing you the difference between a good exercise technique and a bad one, making it a great variable for classification.

------------------------------------

My experience tells me that you need not touch the training set, the only thing which is important is that the variables and type of the variables in the training set should match in both the test and training set i.e. you can have 160 variables in the test set and 60 in the training set and predictions will work. What you cannot have is for e.g. say variable X of type numeric in the training set and the same variable X as integer or factor in the test set.

and look that all are the same, i.e. factor variables

some removed rows with variable window == "yes" and empty columns

Try predict.train() instead predict()

----------------------------------------

rattle()    #what a great data-cleaning and mining tool
Latticist and GGobi   packages: also cool!

-----------------------------------------

As for rendering images inside Rmd files on github, I don't think that's possible. However, images do render if you upload the md file that corresponds to your Rmd along with the image files (keeping the folder structure that knitr generates). The caveat is that you need to tell knitr to output images files separately. By default recent RStudio versions set up knitr to embed images in the output html. This breaks the displaying of images on github. So you need to tell RStudio to set up knitr to output images as separate files that you can then upload. Two settings in the Rmd document prologue (the initial paragraph between ----- marks) control image and md file generation: self_contained = FALSE, keep_md = TRUE

-------------------------------------------
create a gh-page

create a html file xxx.html in your project/folder
git branch gh-pages
git checkout gh-pages
git push origin gh-pages
then just put an empty .nojekyll file in the branch
git add .nojekyll
git commit -a -m "added .nojekyll"
git push origin gh-pages
find your presentation http://selfman2.github.io/folder/xxx.html

-----------------------------------------

data cleaning:

I also noticed that the dataset has two flavors of missing, you can handle these in read.csv with something like this:
na.strings=c("","NA")

Also consider the #DIV/0!, which can change each variable mode


Without trying to give away too much, there is common theme in the records that do NOT have NA. It requires using a filter on another field, which I believe should be done based on what those records appear to contain and because none of the values in the test set have the value that I'm filtering out.

When I filter out those records, many fields are either all blank or all NA, and then it's obvious that they should be ignored/removed.

What about the na.action = na.omit

http://stackoverflow.com/questions/7330915/removing-columns-with-missing-values

Without giving too much away...I encourage you to read the supporting paper accompanying the initial project that generated the data. In that paper, it describes how the researchers used varying time windows to capture some interesting facts about the subjects' performance.

You should not remove rows from the training set. The "yes" rows are not NA across all predictors. I encourage you to think carefully about what you are giving up by removing observations vs. what you are getting by dropping variables that are plagued by NAs.



Well, I will tell you my point . There is something like the " 80/20 Paretto rule " . You have started with a dataset containing 160 variables, when dropping all the column with "NA's" you shall have roughly about 60 variables under way. But even there is something " too much " in you dataset : 0.2*160 = 32 variables.

I do not claim to be always right, but only practical . In the real world it often works like that.

Consider yourself :
1 ) After getting your 60 vars, isolate the numerical ones and detect collinearities , at least by R square. Before applying anything other.

2)  Keep your model understandable, which a forest of variables hardly can be.


I suggest to see Lecture 5 - Week 2: Basic Preprocessing.




    Project Data Set / Original Data Set: both data sets must be clean because:

        The researchers included on the data sets same calculations like kurtosis, average, maximal, minimal values, etc.

        This calculations were made for groups of measurements define by the column “New Window” to filter some noise.

        This is the cause of having “NA” Values

        Same calculations made by the researchers, are not necessary wrong, but not in the right column (for example calculate by yourself  max_roll: maximal value for the roll position for a window of measurements)

    I’m not saying that the conclusions of the researchers are wrong. I only say, that the data set must be clean before using it.

    Although I did not finish yet the cleaning of the data sets, as initial conclusions:

        All columns with calculations done by researchers must be deleted

        Your tidy data set should have the following columns

            user_name

            classe

            raw_timestamp_part_1 / part_2: times for measurements (use POSIXct)

            new_window - num_window: to see when a group of measurements start / finish.

            48 columns with the sensor measurements.

    It is possible that finally the data set do not need the columns “raw_timestamp_part_1 / part_2” and / or new – num_window, but at least 50 columns defined by the “user_name”, “classe”, and 48 columns with the sensor measurements.

Finally, there is a reason for deleting “NA” values and some columns.





Until now I have the following columns: “user_name”, “classe”, “new_window”, “num_window”, 48 sensor data and:

    From the “raw_timestamp_part_1” with POSIXct, I obtained date and time (assume as starting time)



According with this, I will ignore:

    Time and treat measurements as independent observations as Arho said

    user_name

    I will work with the 12 x 4 sensors = 48 variables




I had the same question/concern. If you read the cited paper, the authors ended up picking 17 features based on summary statistics calculated at the end of each sliding window. These seem to be the columns with e.g. mean, variance, skew etc. in their names. These summary stats columns contain valid (non-NA) values only for rows which have new_window = 'yes' (presumably that's the marker for the end of each sliding window). However, the testing dataset only comes with raw data, with none of the summary stats (new_window == 'no' for all 20 rows) This means we are forced to drop them off the training set. Thoughts?





-----------------------------------------

You should keep track of your cross-validated errors and use whatever algorithm produces the most accurate predictions


I should have been more clear. I did settle on one final algorithm (I didn't end up doing any prediction stacking or anything like that). What I mean is that I trained the data on two different algorithms and stopped at that point because the send one I chose had 99% cross-validated out-of-sample error.

I can confirm that I used only approaches learned from the lectures, with very slight tweaks to the tuning parameters.

So my suggestion to you is to work through various classification techniques and compare the out-of-sample (cross-validated) error that each yields. 

Using the nzv from caret per user gives some interesting results as well, seems like we have some defect sensors in the set. It is a good question if one should retain data with measurement problems in the training set.

---------------------------

preprocessing and imputation

preObj <- preProcess(train.pre.df[,impute_cols],method="knnImpute")
train.pre2.df <- predict(preObj, train.pre.df[,impute_cols])
----------------------------





